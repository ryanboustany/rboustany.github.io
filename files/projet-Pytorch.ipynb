{"cells":[{"cell_type":"markdown","metadata":{"id":"4qiDHBdFL7Hx"},"source":["# Some PyTorch Optimization Methods\n","\n","## Instructions\n","\n","This exam aims at evaluating your understanding and skills in implementing optimization algorithms in PyTorch. Specifically, let us consider methods that correspond to the following Ordinary Differential Equations (ODEs):\n","\n","$$\n","x'(t) = -\\nabla f(x(t)), \\quad y'(t) = -\\nabla^2 f(y(t))^{-1} \\nabla f(y(t))\n","$$\n","\n","We focus exclusively on a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ defined as\n","$$\n","f(\\mathbf{x}) = \\frac{1}{2} \\langle A\\mathbf{x}, \\mathbf{x} \\rangle + \\exp(\\|\\mathbf{x}\\|_2) + \\sum_{i=1}^{d} \\mathbf{x}_i,\n","$$\n","\n","where $\\mathbf{x} \\in \\mathbb{R}^d$ and $A$ is a given $d \\times d$ matrix. The term $\\frac{1}{2} \\langle A\\mathbf{x}, \\mathbf{x} \\rangle$ represents a quadratic form, which is convex when the matrix $A$ is positive definite. The term $\\exp(\\|\\mathbf{x}\\|_2)$ is the exponential of the Euclidean norm of $ \\mathbf{x}$, which is strictly convex. Lastly, $\\sum_{i=1}^{d} \\mathbf{x}_i$ is the sum of the elements of $\\mathbf{x}$, which adds a linear component to the function. The interplay of these terms presents an interesting challenge for optimization algorithms, particularly in the presence of the non-linear exponential term.\n","\n","\n","**Please read each question carefully and complete the tasks. Implement your solutions in a Google Colab notebook. Once you have completed the exercises, make sure to share the URL of your Google Colab notebook with us.**\n","\n","**IMPORTANT Requirement:**\n","\n","1. Make sure to set your Colab notebook sharing settings to 'Anyone with the link can view' before submitting the URL.\n","\n","2. Don't work on the notebook you've been given, but make a copy and then create your own share link.\n","  \n","3. Clearly comment your code to explain the logic behind your implementation steps."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1698947572855,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"aho9vGmYL6Rk"},"outputs":[],"source":["import torch\n","from torch.autograd import grad\n","from torch.autograd.functional import jacobian\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","torch.manual_seed(42)\n","torch.set_default_dtype(torch.float64)"]},{"cell_type":"markdown","metadata":{"id":"pZGgqqKjIF0n"},"source":["## Question 1: Implementing the function\n","\n","\n","$f(\\mathbf{x}) = \\frac{1}{2} \\langle A\\mathbf{x}, \\mathbf{x} \\rangle + \\exp(\\|\\mathbf{x}\\|_2) + \\sum_{i=1}^{d} \\mathbf{x}_i$, where $d$ is the dimension of the vector $\\mathbf{x}$.\n","\n","1. Implement a PyTorch function named `f(x, A)` that computes the function $f$."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1698947572855,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"ESsOwG4kNHFs"},"outputs":[],"source":["def f(x, A):\n","    # TODO\n","    return"]},{"cell_type":"markdown","metadata":{"id":"VOqGJR2-NNEZ"},"source":["## Question 2: Implementing Gradient Descent (GD) as an Approximation to the ODE $x'(t)$\n","\n","Your task is to implement Gradient Descent (GD) as an approximation to the Ordinary Differential Equation (ODE) $x'(t) = -\\nabla f(x(t))$. This ODE characterizes the continuous-time dynamics of optimizing the function $f$.\n","\n","The curve is approximated by a sequence\n","$$\\frac{x_{k+1}-x_k}{\\delta}=-\\nabla f(x_k), k\\geq 0,$$\n","where $\\delta>$ is a time increment parameter.\n","\n","1. Write a function named `gd_optimization` to perform GD optimization. The function signature should be as follows:\n","    ```python\n","    def gd_optimization(f, x_init, A, lr, max_iter):\n","        # your code here\n","    ```\n","    - **f**: The objective function to be minimized, specified as a callable function.\n","    - **x_init**: The initial point for the optimization, specified as a PyTorch tensor of shape \\( (d, ) \\).\n","    - **A**: The coefficient matrix in the least squares function, specified as a PyTorch tensor of shape \\( (d, d) \\).\n","    - **lr**: The learning rate, specified as a float.\n","    - **max_iter**: The maximum number of iterations, specified as an integer.\n","\n","    Provide comprehensive comments to clarify each implementation step.\n","\n","    Hint : you can use `torch.autograd.grad`.\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698947572855,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"qKqX8_y-RUCr"},"outputs":[],"source":["def gd_optimization(f, x_init, A, lr, max_iter):\n","    x = #TODO\n","    losses = []\n","\n","    for i in range(max_iter):\n","        loss = #TODO\n","        losses.append(loss.item())\n","\n","        # Compute the gradient\n","        grad_values = #TODO\n","\n","        x.data -= #TODO\n","\n","    return losses\n"]},{"cell_type":"markdown","metadata":{"id":"_AQd1d8tRjwL"},"source":["## Question 3: Implementing Newton's Method\n","\n","Implement Newton's optimization algorithm as a discretization of the ODE $ y'(t) $. The curve is approximated by a sequence\n","$$\\frac{y_{k+1}-y_k}{\\delta}=-\\nabla^2f(x_k)^{-1}\\nabla f(y_k), \\:k\\geq 0,$$\n","where $\\delta>$ is a time increment parameter.\n","1. Write a function named `newton_optimization(f, x_init, A, lr, max_iter)` to perform Newton's optimization.\n","2. Provide comments detailing how you calculate the Hessian matrix and how it relates to the ODE $ y'(t) $.\n","\n","Hint : you can use `torch.autograd.functional.jacobian` and `torch.autograd.grad`.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1698947572855,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"PxFsTtIiSFy0"},"outputs":[],"source":["def newton_optimization(f, x_init, A, lr=1.0, max_iter=100):\n","    x = #TODO\n","    losses = []\n","\n","    for i in range(max_iter):\n","        loss = #TODO\n","        losses.append(loss.item())\n","\n","        # Compute the gradient\n","        grad_values = #TODO\n","\n","        # Compute the Hessian matrix\n","        hessian_matrix = #TODO\n","\n","        # Attempt to invert the Hessian matrix\n","        try:\n","            hessian_inv = #TODO\n","        except RuntimeError as e:\n","            print(f\"An error occurred while inverting the Hessian at iteration {i+1}: {e}\")\n","            break\n","\n","        # Update the parameters\n","        x.data -= #TODO\n","\n","    return losses"]},{"cell_type":"markdown","metadata":{"id":"fwOv_VIkSKql"},"source":["## Question 4: Performance Comparison\n","\n","1. Compare the performance of your GD and Newton's method implementations. Plot the loss over iterations for each method.  "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1698947572856,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"CE_Ew18xcGnw"},"outputs":[],"source":["def plot_loss_over_iterations(losses_dict, title):\n","    plt.figure(figsize=(12, 8))\n","\n","    for label, losses in losses_dict.items():\n","        plt.plot(losses, label=label, marker='o' if 'Newton' in label else 'x', linestyle='-' if 'Newton' in label else '--')\n","\n","    plt.xlabel('Iteration')\n","    plt.ylabel('Loss')\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True)\n","    plt.yscale('symlog')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1698947572856,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"Gif_iETDDZ0R"},"outputs":[],"source":["# Initial point\n","d = 50\n","A = torch.randn((d,d))/d\n","x_init = torch.randn(d)/d"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1042,"status":"ok","timestamp":1698947573895,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"8B0KlQu5cPeK"},"outputs":[],"source":["# Learning rates to experiment with\n","lrs_gd = [0.001,0.01, 1.0]\n","lrs_newton = [0.001, 0.01, 1.0]\n","\n","\n","# Run Newton's and SGD optimization\n","losses = {}\n","for lr in lrs_gd:\n","    losses[f'SGD-lr={lr}'] = gd_optimization(f, x_init, A, lr=lr, max_iter=25)\n","for lr in lrs_newton:\n","    losses[f'Newton-lr={lr}'] = # TODO\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":718},"executionInfo":{"elapsed":609,"status":"ok","timestamp":1698947574498,"user":{"displayName":"Ryan Boustany","userId":"06123842114357382977"},"user_tz":-60},"id":"Z1vQcTE6DZ0S","outputId":"e9da7bef-3a15-499b-f4fe-64a657e4caca"},"outputs":[],"source":["# Plot the loss over iterations for different learning rates\n","plot_loss_over_iterations(losses, 'Newton vs SGD')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
